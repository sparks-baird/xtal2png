
@article{goodallPredictingMaterialsProperties2019,
  title     = {Predicting Materials Properties without Crystal Structure: {{Deep}} Representation Learning from Stoichiometry},
  author    = {Goodall, Rhys E. A. and Lee, Alpha A.},
  year      = {2019},
  publisher = {{arXiv}},
  doi       = {10.48550/ARXIV.1910.00617},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computational Physics (physics.comp-ph),FOS: Computer and information sciences,FOS: Physical sciences,Machine Learning (cs.LG),Materials Science (cond-mat.mtrl-sci)}
}

@article{goodallPredictingMaterialsProperties2020,
  title         = {Predicting Materials Properties without Crystal Structure: {{Deep}} Representation Learning from Stoichiometry},
  shorttitle    = {Predicting Materials Properties without Crystal Structure},
  author        = {Goodall, Rhys E. A. and Lee, Alpha A.},
  year          = {2020},
  month         = dec,
  journal       = {Nature Communications},
  volume        = {11},
  number        = {1},
  eprint        = {1910.00617},
  eprinttype    = {arxiv},
  primaryclass  = {cond-mat, physics:physics},
  pages         = {6280},
  issn          = {2041-1723},
  doi           = {10.1038/s41467-020-19964-7},
  abstract      = {Machine learning has the potential to accelerate materials discovery by accurately predicting materials properties at a low computational cost. However, the model inputs remain a key stumbling block. Current methods typically use descriptors constructed from knowledge of either the full crystal structure -- therefore only applicable to materials with already characterised structures -- or structure-agnostic fixed-length representations hand-engineered from the stoichiometry. We develop a machine learning approach that takes only the stoichiometry as input and automatically learns appropriate and systematically improvable descriptors from data. Our key insight is to treat the stoichiometric formula as a dense weighted graph between elements. Compared to the state of the art for structure-agnostic methods, our approach achieves lower errors with less data.},
  archiveprefix = {arXiv},
  langid        = {english},
  keywords      = {Computer Science - Machine Learning,Condensed Matter - Materials Science,Physics - Computational Physics},
  file          = {C\:\\Users\\sterg\\Zotero\\storage\\IF6WR9YR\\Goodall and Lee - 2020 - Predicting materials properties without crystal st.pdf}
}

@misc{kipfSemisupervisedClassificationGraph2016,
  title     = {Semi-Supervised Classification with Graph Convolutional Networks},
  author    = {Kipf, Thomas N. and Welling, Max},
  year      = {2016},
  publisher = {{arXiv}},
  doi       = {10.48550/ARXIV.1609.02907},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@article{ongPythonMaterialsGenomics2013,
  title      = {Python {{Materials Genomics}} (Pymatgen): {{A}} Robust, Open-Source Python Library for Materials Analysis},
  shorttitle = {Python {{Materials Genomics}} (Pymatgen)},
  author     = {Ong, Shyue Ping and Richards, William Davidson and Jain, Anubhav and Hautier, Geoffroy and Kocher, Michael and Cholia, Shreyas and Gunter, Dan and Chevrier, Vincent L. and Persson, Kristin A. and Ceder, Gerbrand},
  year       = {2013},
  month      = feb,
  journal    = {Computational Materials Science},
  volume     = {68},
  pages      = {314--319},
  issn       = {09270256},
  doi        = {10.1016/j.commatsci.2012.10.028},
  abstract   = {We present the Python Materials Genomics (pymatgen) library, a robust, open-source Python library for materials analysis. A key enabler in high-throughput computational materials science efforts is a robust set of software tools to perform initial setup for the calculations (e.g., generation of structures and necessary input files) and post-calculation analysis to derive useful material properties from raw calculated data. The pymatgen library aims to meet these needs by (1) defining core Python objects for materials data representation, (2) providing a well-tested set of structure and thermodynamic analyses relevant to many applications, and (3) establishing an open platform for researchers to collaboratively develop sophisticated analyses of materials data obtained both from first principles calculations and experiments. The pymatgen library also provides convenient tools to obtain useful materials data via the Materials Project's REpresentational State Transfer (REST) Application Programming Interface (API). As an example, using pymatgen's interface to the Materials Project's RESTful API and phasediagram package, we demonstrate how the phase and electrochemical stability of a recently synthesized material, Li4SnS4, can be analyzed using a minimum of computing resources. We find that Li4SnS4 is a stable phase in the Li\textendash Sn\textendash S phase diagram (consistent with the fact that it can be synthesized), but the narrow range of lithium chemical potentials for which it is predicted to be stable would suggest that it is not intrinsically stable against typical electrodes used in lithium-ion batteries.},
  langid     = {english},
  file       = {C\:\\Users\\sterg\\Zotero\\storage\\GUGQ4GNV\\Ong et al. - 2013 - Python Materials Genomics (pymatgen) A robust, op.pdf}
}

@misc{Pythonista2021,
  title      = {Pythonista},
  year       = {2021},
  month      = nov,
  journal    = {Wiktionary},
  copyright  = {Creative Commons Attribution-ShareAlike License},
  langid     = {english},
  annotation = {Page Version ID: 64674604},
  file       = {C\:\\Users\\sterg\\Zotero\\storage\\8AMZF6UQ\\Pythonista.html}
}

@article{renInvertibleCrystallographicRepresentation2022a,
  title    = {An Invertible Crystallographic Representation for General Inverse Design of Inorganic Crystals with Targeted Properties},
  author   = {Ren, Zekun and Tian, Siyu Isaac Parker and Noh, Juhwan and Oviedo, Felipe and Xing, Guangzong and Li, Jiali and Liang, Qiaohao and Zhu, Ruiming and Aberle, Armin G. and Sun, Shijing and Wang, Xiaonan and Liu, Yi and Li, Qianxiao and Jayavelu, Senthilnath and Hippalgaonkar, Kedar and Jung, Yousung and Buonassisi, Tonio},
  year     = {2022},
  month    = jan,
  journal  = {Matter},
  volume   = {5},
  number   = {1},
  pages    = {314--335},
  issn     = {2590-2385},
  doi      = {10.1016/j.matt.2021.11.032},
  abstract = {Realizing general inverse design could greatly accelerate the discovery of new materials with user-defined properties. However, state-of-the-art generative models tend to be limited to a specific composition or crystal structure. Herein, we present a framework capable of general inverse design (not limited to a given set of elements or crystal structures), featuring a generalized invertible representation that encodes crystals in both real and reciprocal space, and a property-structured latent space from a variational autoencoder (VAE). In three design cases, the framework generates 142 new crystals with user-defined formation energies, bandgap, thermoelectric (TE) power factor, and combinations thereof. These generated crystals, absent in the training database, are validated by first-principles calculations. The success rates (number of first-principles-validated target-satisfying crystals/number of designed crystals) ranges between 7.1\% and 38.9\%. These results represent a significant step toward property-driven general inverse design using generative models, although practical challenges remain when coupled with experimental synthesis.},
  langid   = {english},
  keywords = {general inverse design,generalized crystallographic representation,generative model,invertible crystallographic representation,machine learning,property-structured latent space,solid-state materials,thermoelectrics,variational autoencoder},
  file     = {C\:\\Users\\sterg\\Zotero\\storage\\C597QIUA\\Ren et al_2022_An invertible crystallographic representation for general inverse design of.pdf;C\:\\Users\\sterg\\Zotero\\storage\\3A7B58GN\\S2590238521006251.html}
}

@misc{riebesellPymatviz2022,
  title     = {Pymatviz},
  author    = {Riebesell, Janosh},
  year      = {2022},
  month     = jun,
  abstract  = {A toolkit for visualizations in materials informatics.},
  copyright = {MIT},
  keywords  = {data-visualization,machine-learning,materials-informatics,matplotlib,plotly,plots,uncertainty,uncertainty-calibration}
}

@misc{sahariaPaletteImagetoImageDiffusion2022,
  title         = {Palette: {{Image-to-Image Diffusion Models}}},
  shorttitle    = {Palette},
  author        = {Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris A. and Ho, Jonathan and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  year          = {2022},
  month         = may,
  number        = {arXiv:2111.05826},
  eprint        = {2111.05826},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  abstract      = {This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results.},
  archiveprefix = {arXiv},
  langid        = {english},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file          = {C\:\\Users\\sterg\\Zotero\\storage\\ICZ3654S\\Saharia et al. - 2022 - Palette Image-to-Image Diffusion Models.pdf}
}

@misc{vaswaniAttentionAllYou2017,
  title         = {Attention {{Is All You Need}}},
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year          = {2017},
  month         = dec,
  number        = {arXiv:1706.03762},
  eprint        = {1706.03762},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  abstract      = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  langid        = {english},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file          = {C\:\\Users\\sterg\\Zotero\\storage\\2TR7873X\\Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@article{wangCompositionallyRestrictedAttentionbased2021,
  title    = {Compositionally Restricted Attention-Based Network for Materials Property Predictions},
  author   = {Wang, Anthony Yu-Tung and Kauwe, Steven K. and Murdock, Ryan J. and Sparks, Taylor D.},
  year     = {2021},
  month    = dec,
  journal  = {npj Computational Materials},
  volume   = {7},
  number   = {1},
  pages    = {77},
  issn     = {2057-3960},
  doi      = {10.1038/s41524-021-00545-1},
  abstract = {Abstract             In this paper, we demonstrate an application of the Transformer self-attention mechanism in the context of materials science. Our network, the Compositionally Restricted Attention-Based network (), explores the area of structure-agnostic materials property predictions when only a chemical formula is provided. Our results show that 's performance matches or exceeds current best-practice methods on nearly all of 28 total benchmark datasets. We also demonstrate how 's architecture lends itself towards model interpretability by showing different visualization approaches that are made possible by its design. We feel confident that  and its attention-based framework will be of keen interest to future materials informatics researchers.},
  langid   = {english},
  file     = {C\:\\Users\\sterg\\Zotero\\storage\\APW9NK4X\\Wang et al. - 2021 - Compositionally restricted attention-based network.pdf}
}

@article{wangCompositionallyrestrictedAttentionbasedNetwork2020,
  title     = {Compositionally-Restricted Attention-Based Network for Materials Property Prediction},
  author    = {Wang, Anthony and Kauwe, Steven and Murdock, Ryan and Sparks, Taylor},
  year      = {2020},
  month     = feb,
  publisher = {{American Chemical Society (ACS)}},
  doi       = {10.26434/chemrxiv.11869026.v1}
}

@article{weiningerSMILESChemicalLanguage1988,
  title     = {{{SMILES}}, a Chemical Language and Information System. 1. {{Introduction}} to Methodology and Encoding Rules},
  author    = {Weininger, David},
  year      = {1988},
  month     = feb,
  journal   = {Journal of Chemical Information and Computer Sciences},
  volume    = {28},
  number    = {1},
  pages     = {31--36},
  publisher = {{American Chemical Society}},
  issn      = {0095-2338},
  doi       = {10.1021/ci00057a005},
  file      = {C\:\\Users\\sterg\\Zotero\\storage\\JDGWYLHJ\\Weininger_1988_SMILES, a chemical language and information system.pdf;C\:\\Users\\sterg\\Zotero\\storage\\ZL2984XI\\ci00057a005.html}
}

@article{xieCrystalDiffusionVariational2022,
  title         = {Crystal {{Diffusion Variational Autoencoder}} for {{Periodic Material Generation}}},
  author        = {Xie, Tian and Fu, Xiang and Ganea, Octavian-Eugen and Barzilay, Regina and Jaakkola, Tommi},
  year          = {2022},
  month         = mar,
  journal       = {arXiv:2110.06197 [cond-mat, physics:physics]},
  eprint        = {2110.06197},
  eprinttype    = {arxiv},
  primaryclass  = {cond-mat, physics:physics},
  abstract      = {Generating the periodic structure of stable materials is a long-standing challenge for the material design community. This task is difficult because stable materials only exist in a low-dimensional subspace of all possible periodic arrangements of atoms: 1) the coordinates must lie in the local energy minimum defined by quantum mechanics, and 2) global stability also requires the structure to follow the complex, yet specific bonding preferences between different atom types. Existing methods fail to incorporate these factors and often lack proper invariances. We propose a Crystal Diffusion Variational Autoencoder (CDVAE) that captures the physical inductive bias of material stability. By learning from the data distribution of stable materials, the decoder generates materials in a diffusion process that moves atomic coordinates towards a lower energy state and updates atom types to satisfy bonding preferences between neighbors. Our model also explicitly encodes interactions across periodic boundaries and respects permutation, translation, rotation, and periodic invariances. We significantly outperform past methods in three tasks: 1) reconstructing the input structure, 2) generating valid, diverse, and realistic materials, and 3) generating materials that optimize a specific property. We also provide several standard datasets and evaluation metrics for the broader machine learning community.},
  archiveprefix = {arXiv},
  langid        = {english},
  keywords      = {Computer Science - Machine Learning,Condensed Matter - Materials Science,Physics - Computational Physics},
  file          = {C\:\\Users\\sterg\\Zotero\\storage\\VBQ9U4WF\\Xie et al_2022_Crystal Diffusion Variational Autoencoder for Periodic Material Generation.pdf}
}

@article{xieCrystalGraphConvolutional2017,
  title     = {Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties},
  author    = {Xie, Tian and Grossman, Jeffrey C.},
  year      = {2017},
  publisher = {{arXiv}},
  doi       = {10.48550/ARXIV.1710.10324},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {FOS: Physical sciences,Materials Science (cond-mat.mtrl-sci)}
}

@article{xieCrystalGraphConvolutional2018,
  title         = {Crystal {{Graph Convolutional Neural Networks}} for an {{Accurate}} and {{Interpretable Prediction}} of {{Material Properties}}},
  author        = {Xie, Tian and Grossman, Jeffrey C.},
  year          = {2018},
  month         = apr,
  journal       = {Physical Review Letters},
  volume        = {120},
  number        = {14},
  eprint        = {1710.10324},
  eprinttype    = {arxiv},
  primaryclass  = {cond-mat},
  pages         = {145301},
  issn          = {0031-9007, 1079-7114},
  doi           = {10.1103/PhysRevLett.120.145301},
  abstract      = {The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with \$10\^4\$ data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.},
  archiveprefix = {arXiv},
  keywords      = {Condensed Matter - Materials Science},
  file          = {C\:\\Users\\sterg\\Zotero\\storage\\SCUJG42Y\\Xie_Grossman_2018_Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable.pdf;C\:\\Users\\sterg\\Zotero\\storage\\SJQ6MXAE\\1710.html}
}

@misc{xtal2png,
  title     = {Xtal2png: {{Encode}}/Decode a Crystal Structure to/from a Grayscale {{PNG}} Image for Direct Use with Image-Based Machine Learning Models Such as {{Palette}}.},
  author    = {Baird, Sterling G. and Sayeed, Hasan M.},
  year      = {2022},
  publisher = {{GitHub}}
}


@misc{selfies,
  doi       = {10.48550/ARXIV.2204.00056},
  url       = {https://arxiv.org/abs/2204.00056},
  author    = {Krenn, Mario and Ai, Qianxiang and Barthel, Senja and Carson, Nessa and Frei, Angelo and Frey, Nathan C. and Friederich, Pascal and Gaudin, Théophile and Gayle, Alberto Alexander and Jablonka, Kevin Maik and Lameiro, Rafael F. and Lemm, Dominik and Lo, Alston and Moosavi, Seyed Mohamad and Nápoles-Duarte, José Manuel and Nigam, AkshatKumar and Pollice, Robert and Rajan, Kohulan and Schatzschneider, Ulrich and Schwaller, Philippe and Skreta, Marta and Smit, Berend and Strieth-Kalthoff, Felix and Sun, Chong and Tom, Gary and von Rudorff, Guido Falk and Wang, Andrew and White, Andrew and Young, Adamo and Yu, Rose and Aspuru-Guzik, Alán},
  keywords  = {Chemical Physics (physics.chem-ph), Machine Learning (cs.LG), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {SELFIES and the future of molecular string representations},
  publisher = {arXiv},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}
